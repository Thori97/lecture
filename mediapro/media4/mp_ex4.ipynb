{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4回課題：夏目漱石の「吾輩は猫である」っぽい文を生成してみよう！\n",
    "\n",
    "**課題は結果が出力されている状態で保存して提出してください！**\n",
    "\n",
    "## 課題１：分かち書き文の生成\n",
    "\n",
    "`text/wagahaiwa_nekodearu_org.txt`には、夏目漱石の小説「吾輩は猫である」の本文が記録されています。   \n",
    "このテキストを半角スペース区切りの分かち書き文に変換して、`text/wagahaiwa_nekodearu_wakati.txt`に保存してください。  \n",
    "なお、改行は削除せずにそのまま残してしておいてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吾輩 は 猫 で ある 。 名前 は まだ 無い 。']\n"
     ]
    }
   ],
   "source": [
    "test_data = open(\"text/wagahaiwa_nekodearu_org.txt\", \"r\")\n",
    "test_data2 = open(\"text/wagahaiwa_nekodearu_wakati.txt\", \"w\")\n",
    "\n",
    "contents = test_data.read()\n",
    "\n",
    "\n",
    "sentences = contents.split()\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "wakatis = []\n",
    "t = Tokenizer() # <== これは辞書を読み込み、解析プログラムを起動状態にします。メモリを食うので、１つのプログラムで１回だけ実行して、あとは使いまわしてください。\n",
    "for sentence in sentences:\n",
    "    tokens = t.tokenize(sentence, wakati=True)\n",
    "    wakati= ''\n",
    "    for token in tokens:\n",
    "        wakati += token + ' '\n",
    "    wakati = wakati.rstrip()\n",
    "    test_data2.write(wakati)\n",
    "    wakatis.append(wakati)\n",
    "\n",
    "test_data.close()\n",
    "test_data2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "冒頭の1文を出力してください。  \n",
    "次のような文が出てきたら正解です。  \n",
    "```吾輩 は 猫 で ある 。 名前 は まだ 無い 。 ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吾輩 は 猫 で ある 。 名前 は まだ 無い 。']\n"
     ]
    }
   ],
   "source": [
    "print(wakatis[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題２：ランダム文生成\n",
    "\n",
    "下のセルで、課題１で作成した`text/wagahaiwa_nekodearu_wakati.txt`を入力として、統計的tri-gramモデルを学習してください。  \n",
    "学習モデルの変数名は`lm`とします。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.util import ngrams\n",
    "\n",
    "file = 'text/wagahaiwa_nekodearu_wakati.txt'\n",
    "\n",
    "words = [('<BOP> ' + l + ' <EOP>').split() for l in open(file, 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "\n",
    "# 読み込んだ小説集の語彙（異なり単語）を収集\n",
    "# Vocabularyは1次元のリストを受け取るが、wordsは2次元のリストなので、\n",
    "# wordsを内包表記で2次元から1次元に変換してからVocabularyに渡しています\n",
    "vocab = Vocabulary([item for sublist in words for item in sublist])\n",
    "\n",
    "# 語彙の一覧を表示させたいなら下4行のコメントを有効にする\n",
    "'''num = 0\n",
    "for v in sorted(vocab.counts):\n",
    "    print('{:d}\\t{:s}'.format(num,v))\n",
    "    num += 1\n",
    "'''\n",
    "#print('Vocabulary size: ' + str(len(vocab))) # 語彙サイズ（単語の種類数）\n",
    "\n",
    "text_trigrams = [ngrams(word, 3) for word in words] # tri-gramを生成\n",
    "\n",
    "n = 3\n",
    "lm = MLE(order = n, vocabulary = vocab) # 最尤推定法（Maximum Likelihood Estimation)による統計的n-gram言語モデルの準備\n",
    "lm.fit(text_trigrams) # 上で生成したtri-gramを使って言語モデルを学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデル`lm`を使って、以下のセルで「吾輩は」から始まる文を10文生成してください。   \n",
    "生成文は「吾輩は」から始めて、`。`あるいは`」`で終わる一文を基本としますが、より自然な文となるよう、独自の改良を加えても構いません。   \n",
    "**生成文が出力されている状態で保存して提出してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は無論下た向きでたくさんだと振りむく途端に自己が恃みになりました。\n",
      "吾輩は突然鈴木君はやはり音楽の出来るものです。\n",
      "吾輩は心中ひそかに慶賀の意を表しない。「いや、御金を出して刈り込ませて、しばらく考え込んでいる。\n",
      "吾輩はいろいろあるが、主人はたちまち大きな声を聞いて見るとまるで剣もほろろの挨拶をした心持ち好く日の当る所に吶喊しているくらいだ。\n",
      "吾輩は死ぬまで焦ったって片付く事がないから不平だ。\n",
      "吾輩はこれは昨日火事で焚き出されたのだ。\n",
      "吾輩は日本の猫でも何もしない男である。\n",
      "吾輩は今でも残っていたいの勇猛心の凝って骨となり得るの途に窮するかも知れない。\n",
      "吾輩は金田夫婦と見える。\n",
      "吾輩は彼と近付になって昏睡仮死の庸人と相互する以上はいかなる種類の運動がしいものはいかなる事があって、両眼からかけ始めた。\n"
     ]
    }
   ],
   "source": [
    "### ランダム文生成 ####\n",
    "# contextから始まる文を生成\n",
    "\n",
    "# 最初の2単語はいろいろと変えてみましょう\n",
    "\n",
    "for i in range(0, 10):\n",
    "    context = ['吾輩', 'は']\n",
    "    #context = ['水', '生卵']\n",
    "    # contextのうち最後の2単語から次に繋がる確率0じゃない単語をランダムに選ぶ\n",
    "    while True:\n",
    "        w = lm.generate(text_seed=context)\n",
    "        context.append(w) # 選ばれた単語をcontextに連結 \n",
    "        #if w == '。' or w == '」「'or w == '」'or w=='。「':#ここ下の行と選択'。'だけのほうが自然になる？\n",
    "        if w == '。':\n",
    "            sentence = ''\n",
    "            for word in context:\n",
    "                sentence += word\n",
    "            if w == '」「' or w=='。「':\n",
    "                sentence = sentence.rstrip('「')\n",
    "            print(sentence)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たとえば次のような文が生成されます（これは一例です）。\n",
    "```\n",
    "吾輩は文明の咒詛だ。\n",
    "吾輩はにゃあにゃあと甘えるごとく、雲を行くがごとく腹内に誘致する職務を帯びている。\n",
    "吾輩はちょっとむっとする。\n",
    "吾輩は無論しやせずして来た。\n",
    "吾輩はやはり地面の上を摩擦した女で、気の利かぬ主人は一人が「へえアンドレア・デル・サルトが言ったがそれぎれで手掛りがないから、おとなしく独乙皇帝陛下のように述べ立てる。\n",
    "吾輩は謹んで拝見すると云わぬばかりの顔も遠慮なく見るし、Ｔ１Ｔ2……。\n",
    "吾輩は少からざる尊敬をもって反覆読誦したのね。\n",
    "吾輩は年来美学上の利害もないんです。\n",
    "吾輩はこの近辺でやられて、近火の手でさあ。\n",
    "吾輩は波を打った。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
